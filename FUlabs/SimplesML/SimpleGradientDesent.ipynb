{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: SimplesML\n",
    "Given \n",
    "- Equation: $ax^2 + bx + c = y$\n",
    "- Dataset: $x_i, y_i$ in dataset.json\n",
    "\n",
    "Find\n",
    "- $a, b, c$ that fit the equation\n",
    "\n",
    "Solution\n",
    "- $a = 2, b = 4, c = 5$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equation(x, a, b, c):\n",
    "    # Calculate the value of the function with the given parameters\n",
    "    return (a * x ** 2) + (b * x) + c\n",
    "def solution_equation(x):\n",
    "    # The solution of the equation which we want to find\n",
    "    return ( 2 * x ** 2) + (4 * x) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x -654 || y  852821 || 852821  y_solution\n",
      "x -2832 || y  16029125 || 16029125  y_solution\n",
      "x 2702 || y  14612421 || 14612421  y_solution\n",
      "x 4494 || y  40410053 || 40410053  y_solution\n",
      "x 101 || y  20811 || 20811  y_solution\n",
      "x 2015 || y  8128515 || 8128515  y_solution\n",
      "x 4892 || y  47882901 || 47882901  y_solution\n",
      "x -775 || y  1198155 || 1198155  y_solution\n",
      "x -3660 || y  26776565 || 26776565  y_solution\n",
      "x -334 || y  221781 || 221781  y_solution\n",
      "x -3370 || y  22700325 || 22700325  y_solution\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset = json.load(open('dataset_v1.json'))\n",
    "\n",
    "# Observes the first 10 elements of the dataset\n",
    "num_obs = 10\n",
    "for i, (x, y) in enumerate(dataset.items()):\n",
    "    y_solution = solution_equation(int(x))\n",
    "    print(\"x {} || y  {} || {}  y_solution\".format(x, y, y_solution))\n",
    "    if i == num_obs: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the parameters\n",
    "def get_random_parameters():\n",
    "    a = np.random.uniform(low=-10, high=10)\n",
    "    b = np.random.uniform(low=-10, high=10)\n",
    "    c = np.random.uniform(low=-10, high=10)\n",
    "    return a, b, c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss = ((ax^2 + bx + c) - y_t)^2$\n",
    "\n",
    "$grad_a = 2(ax^2 + bx + c - y_t) * x^2$\n",
    "\n",
    "$grad_b = 2(ax^2 + bx + c - y_t) * x$\n",
    "\n",
    "$grad_c = 2(ax^2 + bx + c - y_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the derivative of the function\n",
    "def loss_function(y_true, y_pred):\n",
    "    # Here y_true is the value of the function in the dataset\n",
    "    # y_pred is the value of the function with the parameters\n",
    "    return (y_pred - y_true) ** 2\n",
    "\n",
    "def gradient_loss_function(y_true, y_pred, x):\n",
    "    # Some python libraries have a function to calculate the derivative of the function\n",
    "    # But we will do it manually and return the gradient of the parameters\n",
    "    # based on the derivative of the function, we can calculate the gradient of the parameters\n",
    "    grad_a = 2 * (y_pred - y_true) * (x ** 2)\n",
    "    grad_b = 2 * (y_pred - y_true) * (x)\n",
    "    grad_c = 2 * (y_pred - y_true)\n",
    "    return grad_a, grad_b, grad_c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000 | Loss 3.368866032265830 | a 4.4763760236 | b 0.9729171043 | c 0.0050826952\n",
      "Step 2000 | Loss 0.173027542705219 | a 3.6664069301 | b 0.5273870280 | c -0.8546877752\n",
      "Step 3000 | Loss 0.024330083009278 | a 3.2478414234 | b 0.2883283514 | c -0.8215657023\n",
      "Step 4000 | Loss 0.005262836759447 | a 2.9176623019 | b 0.1630291699 | c -0.7119643310\n",
      "Step 5000 | Loss 0.001252000149521 | a 2.6377805559 | b 0.0968579261 | c -0.6095227299\n",
      "Step 6000 | Loss 0.000294295643783 | a 2.3985609448 | b 0.0611914872 | c -0.5209545212\n",
      "Step 7000 | Loss 0.000064871729325 | a 2.1940118052 | b 0.0413407254 | c -0.4451182124\n",
      "Step 8000 | Loss 0.000012450640131 | a 2.0191631798 | b 0.0297838384 | c -0.3802833082\n",
      "Step 9000 | Loss 0.000001716386064 | a 1.8697407215 | b 0.0226581293 | c -0.3248760155\n",
      "Step 10000 | Loss 0.000000063766672 | a 1.7420674822 | b 0.0179679651 | c -0.2775337350\n",
      "Step 11000 | Loss 0.000000050639282 | a 1.6329882742 | b 0.0146718777 | c -0.2370864209\n",
      "Step 12000 | Loss 0.000000173376892 | a 1.5398004697 | b 0.0122175432 | c -0.2025318279\n",
      "Step 13000 | Loss 0.000000220612561 | a 1.4601916292 | b 0.0103047101 | c -0.1730124370\n",
      "Step 14000 | Loss 0.000000210874264 | a 1.3921844668 | b 0.0087641468 | c -0.1477950325\n",
      "Step 15000 | Loss 0.000000177884175 | a 1.3340889344 | b 0.0074956650 | c -0.1262529220\n",
      "Step 16000 | Loss 0.000000140837914 | a 1.2844608313 | b 0.0064362491 | c -0.1078505816\n",
      "Step 17000 | Loss 0.000000107721570 | a 1.2420662114 | b 0.0055435469 | c -0.0921304545\n",
      "Step 18000 | Loss 0.000000080797485 | a 1.2058508601 | b 0.0047872166 | c -0.0787016311\n",
      "Step 19000 | Loss 0.000000059922059 | a 1.1749141594 | b 0.0041443084 | c -0.0672301569\n",
      "Step 20000 | Loss 0.000000044147373 | a 1.1484867316 | b 0.0035967271 | c -0.0574307427\n",
      "Step 21000 | Loss 0.000000032398977 | a 1.1259113260 | b 0.0031297834 | c -0.0490596767\n",
      "Step 22000 | Loss 0.000000023722292 | a 1.1066264841 | b 0.0027313193 | c -0.0419087671\n",
      "Step 23000 | Loss 0.000000017345553 | a 1.0901525802 | b 0.0023911477 | c -0.0358001666\n",
      "Step 24000 | Loss 0.000000012672633 | a 1.0760798968 | b 0.0021006675 | c -0.0305819498\n",
      "Step 25000 | Loss 0.000000009254134 | a 1.0640584351 | b 0.0018525824 | c -0.0261243356\n",
      "Step 26000 | Loss 0.000000006755849 | a 1.0537892111 | b 0.0016406860 | c -0.0223164594\n",
      "Step 27000 | Loss 0.000000004931168 | a 1.0450168206 | b 0.0014596897 | c -0.0190636161\n",
      "Step 28000 | Loss 0.000000003598947 | a 1.0375230863 | b 0.0013050825 | c -0.0162849044\n",
      "Step 29000 | Loss 0.000000002626484 | a 1.0311216326 | b 0.0011730144 | c -0.0139112155\n",
      "Step 30000 | Loss 0.000000001916719 | a 1.0256532495 | b 0.0010601984 | c -0.0118835135\n",
      "Step 31000 | Loss 0.000000001398727 | a 1.0209819334 | b 0.0009638272 | c -0.0101513677\n",
      "Step 32000 | Loss 0.000000001020708 | a 1.0169915046 | b 0.0008815035 | c -0.0086716980\n",
      "Step 33000 | Loss 0.000000000744847 | a 1.0135827174 | b 0.0008111795 | c -0.0074077038\n",
      "Step 34000 | Loss 0.000000000543539 | a 1.0106707922 | b 0.0007511060 | c -0.0063279483\n",
      "Step 35000 | Loss 0.000000000396637 | a 1.0081833068 | b 0.0006997888 | c -0.0054055772\n",
      "Step 36000 | Loss 0.000000000289437 | a 1.0060583953 | b 0.0006559516 | c -0.0046176501\n",
      "Step 37000 | Loss 0.000000000211211 | a 1.0042432092 | b 0.0006185041 | c -0.0039445706\n",
      "Step 38000 | Loss 0.000000000154126 | a 1.0026926031 | b 0.0005865149 | c -0.0033695986\n",
      "Step 39000 | Loss 0.000000000112470 | a 1.0013680120 | b 0.0005591885 | c -0.0028784340\n",
      "Step 40000 | Loss 0.000000000082073 | a 1.0002364923 | b 0.0005358451 | c -0.0024588612\n",
      "Step 41000 | Loss 0.000000000059891 | a 0.9992699020 | b 0.0005159042 | c -0.0021004450\n",
      "Step 42000 | Loss 0.000000000043704 | a 0.9984442011 | b 0.0004988699 | c -0.0017942713\n",
      "Step 43000 | Loss 0.000000000031892 | a 0.9977388537 | b 0.0004843185 | c -0.0015327252\n",
      "Step 44000 | Loss 0.000000000023272 | a 0.9971363171 | b 0.0004718881 | c -0.0013093018\n",
      "Step 45000 | Loss 0.000000000016983 | a 0.9966216058 | b 0.0004612696 | c -0.0011184444\n",
      "Step 46000 | Loss 0.000000000012393 | a 0.9961819184 | b 0.0004521988 | c -0.0009554063\n",
      "Step 47000 | Loss 0.000000000009043 | a 0.9958063195 | b 0.0004444501 | c -0.0008161325\n",
      "Step 48000 | Loss 0.000000000006599 | a 0.9954854675 | b 0.0004378309 | c -0.0006971591\n",
      "Step 49000 | Loss 0.000000000004816 | a 0.9952113827 | b 0.0004321765 | c -0.0005955271\n",
      "Step 50000 | Loss 0.000000000003514 | a 0.9949772482 | b 0.0004273463 | c -0.0005087090\n",
      "Step 51000 | Loss 0.000000000002564 | a 0.9947772410 | b 0.0004232201 | c -0.0004345454\n",
      "Step 52000 | Loss 0.000000000001871 | a 0.9946063867 | b 0.0004196954 | c -0.0003711918\n",
      "Step 53000 | Loss 0.000000000001365 | a 0.9944604360 | b 0.0004166844 | c -0.0003170726\n",
      "Step 54000 | Loss 0.000000000000996 | a 0.9943357589 | b 0.0004141123 | c -0.0002708417\n",
      "Step 55000 | Loss 0.000000000000727 | a 0.9942292547 | b 0.0004119151 | c -0.0002313495\n",
      "Step 56000 | Loss 0.000000000000531 | a 0.9941382744 | b 0.0004100382 | c -0.0001976136\n",
      "Step 57000 | Loss 0.000000000000387 | a 0.9940605553 | b 0.0004084348 | c -0.0001687950\n",
      "Step 58000 | Loss 0.000000000000283 | a 0.9939941646 | b 0.0004070652 | c -0.0001441770\n",
      "Step 59000 | Loss 0.000000000000206 | a 0.9939374508 | b 0.0004058952 | c -0.0001231472\n",
      "Step 60000 | Loss 0.000000000000150 | a 0.9938890036 | b 0.0004048957 | c -0.0001051828\n",
      "Step 61000 | Loss 0.000000000000110 | a 0.9938476181 | b 0.0004040419 | c -0.0000898368\n",
      "Step 62000 | Loss 0.000000000000080 | a 0.9938122649 | b 0.0004033126 | c -0.0000767277\n",
      "Step 63000 | Loss 0.000000000000058 | a 0.9937820647 | b 0.0004026895 | c -0.0000655293\n",
      "Step 64000 | Loss 0.000000000000043 | a 0.9937562665 | b 0.0004021573 | c -0.0000559632\n",
      "Step 65000 | Loss 0.000000000000031 | a 0.9937342286 | b 0.0004017027 | c -0.0000477915\n",
      "Step 66000 | Loss 0.000000000000023 | a 0.9937154029 | b 0.0004013143 | c -0.0000408108\n",
      "Step 67000 | Loss 0.000000000000017 | a 0.9936993212 | b 0.0004009825 | c -0.0000348477\n",
      "Step 68000 | Loss 0.000000000000012 | a 0.9936855836 | b 0.0004006991 | c -0.0000297537\n",
      "Step 69000 | Loss 0.000000000000009 | a 0.9936738484 | b 0.0004004570 | c -0.0000254022\n",
      "Step 70000 | Loss 0.000000000000006 | a 0.9936638237 | b 0.0004002502 | c -0.0000216850\n",
      "Step 71000 | Loss 0.000000000000005 | a 0.9936552602 | b 0.0004000736 | c -0.0000185096\n",
      "Step 72000 | Loss 0.000000000000003 | a 0.9936479449 | b 0.0003999226 | c -0.0000157971\n",
      "Step 73000 | Loss 0.000000000000003 | a 0.9936416959 | b 0.0003997937 | c -0.0000134799\n",
      "Step 74000 | Loss 0.000000000000002 | a 0.9936363577 | b 0.0003996836 | c -0.0000115005\n",
      "Step 75000 | Loss 0.000000000000001 | a 0.9936317976 | b 0.0003995895 | c -0.0000098096\n",
      "Step 76000 | Loss 0.000000000000001 | a 0.9936279022 | b 0.0003995092 | c -0.0000083652\n",
      "Step 77000 | Loss 0.000000000000001 | a 0.9936245746 | b 0.0003994405 | c -0.0000071313\n",
      "Step 78000 | Loss 0.000000000000001 | a 0.9936217320 | b 0.0003993819 | c -0.0000060772\n",
      "Step 79000 | Loss 0.000000000000000 | a 0.9936193037 | b 0.0003993318 | c -0.0000051768\n",
      "Step 80000 | Loss 0.000000000000000 | a 0.9936172294 | b 0.0003992890 | c -0.0000044076\n",
      "Step 81000 | Loss 0.000000000000000 | a 0.9936154574 | b 0.0003992524 | c -0.0000037506\n",
      "Step 82000 | Loss 0.000000000000000 | a 0.9936139437 | b 0.0003992212 | c -0.0000031893\n",
      "Step 83000 | Loss 0.000000000000000 | a 0.9936126507 | b 0.0003991945 | c -0.0000027098\n",
      "Step 84000 | Loss 0.000000000000000 | a 0.9936115461 | b 0.0003991717 | c -0.0000023003\n",
      "Step 85000 | Loss 0.000000000000000 | a 0.9936106025 | b 0.0003991523 | c -0.0000019504\n",
      "Step 86000 | Loss 0.000000000000000 | a 0.9936097965 | b 0.0003991356 | c -0.0000016515\n",
      "Step 87000 | Loss 0.000000000000000 | a 0.9936091079 | b 0.0003991214 | c -0.0000013962\n",
      "Step 88000 | Loss 0.000000000000000 | a 0.9936085198 | b 0.0003991093 | c -0.0000011781\n",
      "Step 89000 | Loss 0.000000000000000 | a 0.9936080173 | b 0.0003990989 | c -0.0000009918\n",
      "Step 90000 | Loss 0.000000000000000 | a 0.9936075881 | b 0.0003990901 | c -0.0000008326\n",
      "Step 91000 | Loss 0.000000000000000 | a 0.9936072214 | b 0.0003990825 | c -0.0000006966\n",
      "Step 92000 | Loss 0.000000000000000 | a 0.9936069082 | b 0.0003990761 | c -0.0000005805\n",
      "Step 93000 | Loss 0.000000000000000 | a 0.9936066407 | b 0.0003990705 | c -0.0000004813\n",
      "Step 94000 | Loss 0.000000000000000 | a 0.9936064121 | b 0.0003990658 | c -0.0000003965\n",
      "Step 95000 | Loss 0.000000000000000 | a 0.9936062169 | b 0.0003990618 | c -0.0000003241\n",
      "Step 96000 | Loss 0.000000000000000 | a 0.9936060501 | b 0.0003990583 | c -0.0000002623\n",
      "Step 97000 | Loss 0.000000000000000 | a 0.9936059076 | b 0.0003990554 | c -0.0000002095\n",
      "Step 98000 | Loss 0.000000000000000 | a 0.9936057859 | b 0.0003990529 | c -0.0000001643\n",
      "Step 99000 | Loss 0.000000000000000 | a 0.9936056819 | b 0.0003990508 | c -0.0000001258\n"
     ]
    }
   ],
   "source": [
    "# get initial parameters\n",
    "a, b, c = get_random_parameters()\n",
    "# Learning rate\n",
    "lr = 1e-3\n",
    "# The number of times that the algorithm will run\n",
    "steps = 100000\n",
    "# Create an infinity data generator\n",
    "data_generator = itertools.cycle(dataset.items())\n",
    "# Scale the data to 0-1\n",
    "x_max = max([float(x) for x in dataset.keys()])\n",
    "y_max = max([float(y) for y in dataset.values()])\n",
    "for step in range(1,steps):\n",
    "    # Get the next data\n",
    "    x, y_true = next(data_generator)\n",
    "    x, y_true = float(x), float(y_true)\n",
    "    x_scaled = x / x_max\n",
    "    y_true_scaled = y_true / y_max\n",
    "    # Try to predict the value of the function with the parameters\n",
    "    y_predict = equation(x_scaled, a, b, c)\n",
    "    # Calculate the loss function\n",
    "    loss = loss_function(y_true_scaled, y_predict)\n",
    "    # Calculate the gradient of the parameters\n",
    "    grad_a, grad_b, grad_c = gradient_loss_function(y_true_scaled, y_predict, x_scaled)\n",
    "    # Update the parameters\n",
    "    a = a - (lr * grad_a)\n",
    "    b = b - (lr * grad_b)\n",
    "    c = c - (lr * grad_c)\n",
    "    # Logging the result\n",
    "    if step % 1000 == 0:\n",
    "        print(\"Step {} | Loss {:.15f} | a {:.10f} | b {:.10f} | c {:.10f}\".format(step, loss, a, b, c))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_equation(x, a, b, c, x_max, y_max):\n",
    "    # In the training process, we scaled the data to 0-1.\n",
    "    # When we want to predict the value of the function, we need to scale the data to 0-1\n",
    "    # And rescale the result to the original value\n",
    "    # scale the data to 0-1\n",
    "    x_scaled = x / x_max\n",
    "    # calculate the value of the function\n",
    "    y_scaled = equation(x_scaled, a, b, c)\n",
    "    # rescale the result to the original value\n",
    "    y = y_scaled * y_max\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8963381, 8963376.283888731)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = random.randint(1, x_max)\n",
    "y_solution = solution_equation(x)\n",
    "y_predict = inference_equation(x, a, b, c, x_max, y_max)\n",
    "y_solution, y_predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: A little bit more complex\n",
    "Given \n",
    "- Equation: \n",
    "\n",
    "$a_1x_1 + a_2x_2 + b_1 = y_1$ \n",
    "\n",
    "$a_3x_1 + a_4x_2 + b_2 = y_2$\n",
    "\n",
    "$y = (y_1 + y_2) / 2$\n",
    "- Dataset: $(x_{1i}, x_{2i}), (y_{1i}, y_{2i})$ in dataset.json\n",
    "\n",
    "Find\n",
    "- $a_1, a_2, b_1$\n",
    "- $a_3, a_4, b_2$ that fit the equation\n",
    "\n",
    "Solution\n",
    "- $a_1 = 2, a_2 = 4, b_1 = 9$\n",
    "- $a_3 = 4, a_4 = 3, b_2 = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import itertools\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equation(inputs, weights, bias):\n",
    "    # Calculate the value of the function with the given parameters\n",
    "    return np.mean(np.dot(inputs, weights) + bias) # inputs@weights + bias\n",
    "def solution_equation(inputs):\n",
    "    # The solution of the equation which we want to find\n",
    "    x1, x2 = inputs\n",
    "    y1 = 2*x1 + 4*x2 + 9\n",
    "    y2 = 4*x1 + 3*x2 + 5\n",
    "    y = (y1 + y2) / 2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [-2418, -2318] || y  -15360.0 || -15360.0  y_solution\n",
      "x [-1990, -1890] || y  -12578.0 || -12578.0  y_solution\n",
      "x [-2086, -1986] || y  -13202.0 || -13202.0  y_solution\n",
      "x [-2059, -1959] || y  -13026.5 || -13026.5  y_solution\n",
      "x [-4095, -3995] || y  -26260.5 || -26260.5  y_solution\n",
      "x [1755, 1855] || y  11764.5 || 11764.5  y_solution\n",
      "x [-2579, -2479] || y  -16406.5 || -16406.5  y_solution\n",
      "x [3623, 3723] || y  23906.5 || 23906.5  y_solution\n",
      "x [97, 197] || y  987.5 || 987.5  y_solution\n",
      "x [-4441, -4341] || y  -28509.5 || -28509.5  y_solution\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset = json.load(open('dataset_v2.json'))\n",
    "x_set, y_set = dataset['x'], dataset['y']\n",
    "# Observes the first 10 elements of the dataset\n",
    "num_obs = 10\n",
    "for i in range(num_obs):\n",
    "    x = x_set[i]\n",
    "    y = y_set[i]\n",
    "    y_solution = solution_equation(x)\n",
    "    print(\"x {} || y  {} || {}  y_solution\".format(x, y, y_solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialize the parameters\n",
    "def get_random_parameters():\n",
    "    weights = np.random.rand(2,2)\n",
    "    bias = np.random.rand(2,1)\n",
    "    return weights, bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$loss = ((ax^2 + bx + c) - y_t)^2$\n",
    "\n",
    "$grad_a = 2(ax^2 + bx + c - y_t) * x^2$\n",
    "\n",
    "$grad_b = 2(ax^2 + bx + c - y_t) * x$\n",
    "\n",
    "$grad_c = 2(ax^2 + bx + c - y_t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the derivative of the function\n",
    "def loss_function(y_true, y_pred):\n",
    "    # Here y_true is the value of the function in the dataset\n",
    "    # y_pred is the value of the function with the parameters\n",
    "    return (y_pred - y_true) ** 2\n",
    "\n",
    "def gradient_loss_function(y_true, y_pred, x):\n",
    "    # Now we need to calculate the derivative of the matrix\n",
    "    # each element of the matrix will have a derivative\n",
    "    # here we have 4 parameters for weights and 2 parameters for bias\n",
    "    # [a1 a2] [b1]\n",
    "    # [a3 a4] [b2]\n",
    "    # base on chain rule, we can calculate the derivative of the matrix\n",
    "    # loss = (y_pred - y_true) ** 2\n",
    "    # d(loss)/d(y_pred) = 2 * (y_pred - y_true)\n",
    "    # d(loss)/d(a1) = d(loss)/d(y_pred) * d(y_pred)/d(a1)\n",
    "    #               = 2 * (y_pred - y_true) * x1\n",
    "    grad_a1 = 2 * (y_pred - y_true) * x[0]\n",
    "    grad_a2 = 2 * (y_pred - y_true) * x[1]\n",
    "    grad_a3 = 2 * (y_pred - y_true) * x[0]\n",
    "    grad_a4 = 2 * (y_pred - y_true) * x[1]\n",
    "\n",
    "    grad_b1 = 2 * (y_pred - y_true)\n",
    "    grad_b2 = 2 * (y_pred - y_true)\n",
    "    \n",
    "    # Create matrix gradient of the parameters\n",
    "    grads_weights = np.array([[grad_a1, grad_a2], [grad_a3, grad_a4]])\n",
    "    grads_bias = np.array([[grad_b1], [grad_b2]])\n",
    "    return grads_weights, grads_bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99999 | Loss 0.000000000000000 | a_0 0.0135797524 | b_0 0.1946643906\n"
     ]
    }
   ],
   "source": [
    "# get initial parameters\n",
    "weights, bias = get_random_parameters()\n",
    "# Learning rate\n",
    "lr = 1e-3\n",
    "# The number of times that the algorithm will run\n",
    "steps = 100000\n",
    "# Create an infinity data generator\n",
    "data_generator = itertools.cycle(zip(x_set, y_set))\n",
    "# Scale the data to 0-1\n",
    "x_max = np.max(x_set)\n",
    "y_max = np.max(y_set)\n",
    "for step in range(1,steps):\n",
    "    # Get the next data\n",
    "    x, y_true = next(data_generator)\n",
    "    # Convert the data to numpy array\n",
    "    x, y_true = np.array(x), np.array(y_true)\n",
    "    # Scale the data to 0-1\n",
    "    x_scaled = x / x_max\n",
    "    y_true_scaled = y_true / y_max\n",
    "    # Try to predict the value of the function with the parameters\n",
    "    y_predict = equation(x_scaled, weights, bias)\n",
    "    # Calculate the loss function\n",
    "    loss = loss_function(y_true_scaled, y_predict)\n",
    "    # Calculate the gradient of the parameters\n",
    "    grad_weights, grads_bias = gradient_loss_function(y_true_scaled, y_predict, x_scaled)\n",
    "    # Update the parameters\n",
    "    weights = weights - (lr * grad_weights)\n",
    "    bias = bias - (lr * grads_bias)\n",
    "    # Logging the result\n",
    "    # if step % 1000 == 0:\n",
    "print(\"Step {} | Loss {:.15f} | a_0 {:.10f} | b_0 {:.10f}\".format(step, loss, weights[0][0], bias[0][0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_equation(inputs, weights, bias, x_max, y_max):\n",
    "    # In the training process, we scaled the data to 0-1.\n",
    "    # When we want to predict the value of the function, we need to scale the data to 0-1\n",
    "    # And rescale the result to the original value\n",
    "    # scale the data to 0-1\n",
    "    x_scaled = x / x_max\n",
    "    # calculate the value of the function\n",
    "    y_scaled = equation(x_scaled, weights, bias)\n",
    "    # rescale the result to the original value\n",
    "    y = y_scaled * y_max\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23906.5, 23906.5, 23906.500000000007)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the next data\n",
    "x, y_true = next(data_generator)\n",
    "# Convert the data to numpy array\n",
    "x, y_true = np.array(x), np.array(y_true)\n",
    "y_solution = solution_equation(x)\n",
    "y_predict = inference_equation(x, weights, bias, x_max, y_max)\n",
    "y_solution, float(y_true), y_predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FULabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12 | packaged by conda-forge | (default, Oct 26 2021, 06:08:21) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51247d44b39bf149fff136edb48516d1b513897f9bf571a601174ae447a18601"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
